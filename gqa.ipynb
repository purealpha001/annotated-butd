{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OHdC5K3cls6t"
   },
   "source": [
    "# GQA - Bottom-Up Top-Down Attention\n",
    "\n",
    "**gqa.py** is a streamlined port of the [Annotated BUTD](https://github.com/siddk/annotated-butd) repository, to exist as a fully runnable, single-file (for those who prefer to play around with code directly, and see the whole story all at once).\n",
    "\n",
    "It steps through each of the stages of training a Bottom-Up Top-Down (BUTD) model on the GQA Dataset including:\n",
    "\n",
    "\n",
    "*   **Preprocessing**\n",
    "*   **Architecture Definition**\n",
    "*   **Training** (facilitated by [Pytorch-Lightning](https://pytorch-lightning.readthedocs.io/en/latest/))\n",
    "\n",
    "Note that this file only includes streamlined code for the original Bottom-Up Top-Down Model, with the simple product-based fusion operation.\n",
    "\n",
    "For the BUTD-FiLM Model, consult the [Modular Branch](https://github.com/siddk/annotated-butd/blob/modular/src/models/film.py) of the [Annotated BUTD](https://github.com/siddk/annotated-butd) repository -- the code is quite similar.\n",
    "\n",
    "To run this notebook, go to Cell --> Run all.\n",
    "\n",
    "*Final Note: This is a living document, subject to change. Feel free to reach out to me via [skaramcheti@cs.stanford.edu](mailto:skaramcheti@cs.stanford.edu) or [@siddkaramcheti](https://twitter.com/siddkaramcheti) on Twitter if you have questions or comments!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7U_24oPoto7"
   },
   "source": [
    "## Preliminaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jtGjtEYmutQB"
   },
   "source": [
    "### Imports\n",
    "\n",
    "To run the code, we'll need the following dependencies:\n",
    "\n",
    "*   Numpy\n",
    "*   H5py\n",
    "*   PyTorch\n",
    "*   Pytorch-Lightning\n",
    "\n",
    "Amongst others...\n",
    "\n",
    "The following two cells perform the pip installs, and import the corresponding libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "P8bArqG9vO9r",
    "outputId": "144fbdac-3bb9-416f-a24f-4ee23dc77b96"
   },
   "outputs": [],
   "source": [
    "!pip install numpy h5py torch pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FtvDfi_zvbnc"
   },
   "outputs": [],
   "source": [
    "# Various Imports we'll need throughout the Notebook...\n",
    "from argparse import Namespace\n",
    "from datetime import datetime\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import LightningLoggerBase\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "from torch.nn.utils.weight_norm import weight_norm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import base64\n",
    "import csv\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import pytorch_lightning as pl\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qj5kX_YSsbOi"
   },
   "source": [
    "### Downloading Datasets\n",
    "\n",
    "Let's unpack the scripts for [GloVe](https://github.com/siddk/annotated-butd/blob/modular/scripts/glove.sh) and [GQA](https://github.com/siddk/annotated-butd/blob/modular/scripts/gqa.sh) and download the original word vectors, image features, and questions to this folder!\n",
    "\n",
    "**Warning:** Google Colab has a really hard time dealing with large files -- both downloading and unzipping. I highly recommend you import the shared folder into your GDrive ahead of time, and mounting the appropriate directory at the top of the script (so that you don't have to incur the cost of any of the following code).\n",
    "\n",
    "If you're running this from a local .ipynb -- you should be fine, provided you have enough disk space.\n",
    "\n",
    "The GQA Features and Questions take up ~60GB of Disk Space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZOueiB4Gpyr"
   },
   "outputs": [],
   "source": [
    "# Create Data Directory (if doesn't exist)\n",
    "if not os.path.exists('data'):\n",
    "    !mkdir data\n",
    "\n",
    "# Download GloVe Vectors (if not already downloaded)\n",
    "if not os.path.exists('data/GloVe/glove.6B.300d.txt'):\n",
    "    !wget -P data http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    !unzip data/glove.6B.zip -d data/GloVe\n",
    "    !rm data/glove.6B.zip\n",
    "\n",
    "# Download GQA Data -- Features\n",
    "if not os.path.exists('data/GQA-Features/vg_gqa_obj36.tsv') or \\\n",
    "    not os.path.exists('data/GQA-Features/gqa_testdev_obj36.tsv'):\n",
    "    !mkdir data/GQA-Features\n",
    "\n",
    "    # Get GQA Features -- courtesy of our friends at UNC (LXMERT)\n",
    "    !wget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/vg_gqa_imgfeat/vg_gqa_obj36.zip\n",
    "    !wget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/vg_gqa_imgfeat/gqa_testdev_obj36.zip\n",
    "\n",
    "    !unzip vg_gqa_obj36.zip\n",
    "    !unzip gqa_testdev_obj36.zip\n",
    "\n",
    "    !rm vg_gqa_obj36.zip\n",
    "    !rm gqa_testdev_obj36.zip\n",
    "\n",
    "    !mv vg_gqa_imgfeat/vg_gqa_obj36.tsv data/GQA-Features\n",
    "    !mv vg_gqa_imgfeat/gqa_testdev_obj36.tsv data/GQA-Features\n",
    "\n",
    "    !rm -r vg_gqa_imgfeat\n",
    "\n",
    "# Download GQA Data -- Questions\n",
    "if not os.path.exists('data/GQA-Questions/train_balanced_questions.json') or \\\n",
    "    not os.path.exists('data/GQA-Questions/testdev_balanced_questions.json'):\n",
    "    !wget https://nlp.stanford.edu/data/gqa/questions1.3.zip\n",
    "\n",
    "    !unzip questions1.3.zip\n",
    "    !rm questions1.3.zip\n",
    "\n",
    "    !mv *balanced_questions.json data/GQA-Questions\n",
    "    !rm challenge_all_questions.json\n",
    "    !rm submission_all_questions.json\n",
    "    !rm test_all_questions.json\n",
    "    !rm val_all_questions.json\n",
    "    !rm -rf train_all_questions\n",
    "    !rm readme.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEloU7k5JPWt"
   },
   "source": [
    "## Run Parameters\n",
    "\n",
    "In lieu of command line arguments, we'll define all our Run Parameters here -- giving us full control over I/O parameters, as well as specific hyperparameters for our Bottom-Up Top-Down Attention Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTV9P3BvMvL5"
   },
   "outputs": [],
   "source": [
    "# Cast as Namespace for easy access downstream\n",
    "args = Namespace(**{\n",
    "    # Data/Run Parameters\n",
    "    \"run_name\": \"GQA-Streamlined\",          # Change to suit your needs!\n",
    "\n",
    "    \"data\": \"data/\",                        # Where downloaded data is located (relative path)\n",
    "    \"checkpoint\": \"checkpoints/\",           # Where to save model checkpoints and serialized statistics\n",
    "\n",
    "    \"gqa_questions\": \"data/GQA-Questions\",  # Path to GQA Balanced Training/TestDev Set of Questions\n",
    "    \"gqa_features\": \"data/GQA-Features\",    # Path to GQA Object Features\n",
    "    \"gqa_cache\": \"data/GQA-Cache\",          # Path to GQA Cache Directory for storing serialized data (compute once!)\n",
    "    \"glove\": \"data/GloVe/glove.6B.300d.txt\", # Path to GloVe Embeddings File (300-dim)\n",
    "\n",
    "    # BUTD Model Parameters\n",
    "    \"emb_dim\": 300,                         # Word Embedding Dimension --> Should match GloVe (300)\n",
    "    \"emb_dropout\": 0.0,                     # Dropout to apply to Word Embeddings\n",
    "\n",
    "    \"rnn\": \"GRU\",                           # RNN Type for Question Encoder --> one of < 'GRU' | 'LSTM'>\n",
    "    \"rnn_layers\": 1,                        # Number of RNN Stacked Layers (for Question Encoder)\n",
    "    \"bidirectional\": False,                 # Whether or not RNN is Bidirectional (default: False)               \n",
    "    \"q_dropout\": 0.0,                       # RNN Dropout (for Question Encoder)\n",
    "\n",
    "    \"attention_dropout\": 0.2,               # Dropout for Attention Operation (fusing Image + Question)\n",
    "\n",
    "    \"answer_dropout\": 0.5,                  # Dropout to Apply to Answer Classifier\n",
    "\n",
    "    \"hidden\": 1024,                         # Dimensionality of Hidden Layer (Question Encoder & Object Encoder)\n",
    "\n",
    "    \"weight_norm\": True,                    # [IMPORTANT] Boolean whether or not to use weight normalization\n",
    "\n",
    "    # Training Parameters\n",
    "    \"bsz\": 256,                             # Batch Size --> the Bigger the Better\n",
    "    \"epochs\": 15,                           # Number of Training Epochs\n",
    "\n",
    "    \"opt\": \"adamax\",                        # Optimizer for Performing Gradient Updates\n",
    "    \"gradient_clip\": 0.25,                  # Value for Gradient Clipping\n",
    "\n",
    "    \"seed\": 7                               # Random Seed (for Reproducibility)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "as7UrwWTaxMX"
   },
   "source": [
    "Note that we define an argument `gqa_cache` that we use to store serialized/formatted data during the preprocessing step (e.g. HDF5 files). Feel free to change this to a path that is convenient for you (and has enough storage -- this directory can grow large!).\n",
    "\n",
    "The argument `checkpoint` is a path to a checkpoint directory to store model metrics and checkpoints (saved based on best validation accuracy). Feel free to change this as well.\n",
    "\n",
    "Finally, the remaining arguments contain sane defaults for initializing the different parts of the Bottom-Up Top-Down (BUTD) model -- these are not optimized, but seem to work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6gFfhtTbS8m"
   },
   "source": [
    "### Initialization\n",
    "\n",
    "Create Informative Run Name, set Randomness, and \"start\" Training Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "g3MntXqkaftm",
    "outputId": "1a47687a-7ea0-4e8d-96ad-72061f7b9076"
   },
   "outputs": [],
   "source": [
    "# Set Informative Run Name\n",
    "run_name = args.run_name + '-butd' + '-x%d' % args.seed + '+' + datetime.now().strftime('%m-%d-[%H:%M]')\n",
    "print('[*] Starting Train Job in Mode GQA with Run Name: %s' % run_name)\n",
    "\n",
    "# Set Randomness\n",
    "print('[*] Setting Random Seed to %d!' % args.seed)\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fx66lnwLcc3P"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "There are 4 steps to the preprocessing pipeline for GQA:\n",
    "\n",
    "1.   **Preprocessing Question Data**: Involves creating dictionaries of question tokens, for later vectorization.\n",
    "2.   **Preprocessing Answer Data**: Creating Mappings between string answers and their corresponding indices (for final softmax prediction in model's final layer)\n",
    "3.   **Preprocessing Image Features**: Creating an HDF5 file for easy/efficient access to Bottom-Up Object Features for each image, to serve as input to the model.\n",
    "4.   **Dataset Assembly**: Create an official `torch.Dataset` wrapping the VQA Data in an easy-to-batch format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMmsjZ0fdqrh"
   },
   "source": [
    "### 1. Preprocessing Question Data\n",
    "\n",
    "Assemble a Dictionary mapping question tokens to integer indices. Additionally, use the created dictionaries to index and load in GloVe vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7krkNJF7b2Mn"
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self, word2idx=None, idx2word=None):\n",
    "        if word2idx is None:\n",
    "            word2idx = {}\n",
    "        if idx2word is None:\n",
    "            idx2word = []\n",
    "        self.word2idx, self.idx2word = word2idx, idx2word\n",
    "        \n",
    "    @property\n",
    "    def ntoken(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    @property\n",
    "    def padding_idx(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def tokenize(self, sentence, add_word):\n",
    "        sentence = sentence.lower().replace(',', '').replace('.', '').replace('?', '').replace('\\'s', ' \\'s')\n",
    "        words, tokens = sentence.split(), []\n",
    "\n",
    "        if add_word:\n",
    "            for w in words:\n",
    "                tokens.append(self.add_word(w))\n",
    "        else:\n",
    "            for w in words:\n",
    "                tokens.append(self.word2idx.get(w, self.padding_idx))\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LrHqHfiKd9z4"
   },
   "source": [
    "**Note**: It's worth talking about a common design pattern that you'll see throughout this codebase, around utilizing the `gqa_cache` directory to it's fullest potential.\n",
    "\n",
    "As we compute serialized/formatted versions of data (token dictionaries, embedding matrices, HDF5 files), we cache them for future runs to speed up the iteration time.\n",
    "\n",
    "For a research codebase (where speedy iteration is the name of the game), we find this to be a useful practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mCzPS2FHd28d"
   },
   "outputs": [],
   "source": [
    "# Create Dictionary from GQA Question Files, and Initialize GloVe Embeddings from File\n",
    "def gqa_create_dictionary_glove(gqa_q='data/GQA-Questions', glove='data/GloVe/glove.6B.300d.txt', \n",
    "                                cache='data/GQA-Cache'):\n",
    "\n",
    "    dfile, gfile = os.path.join(cache, 'dictionary.pkl'), os.path.join(cache, 'glove.npy')\n",
    "    if os.path.exists(dfile) and os.path.exists(gfile):\n",
    "        with open(dfile, 'rb') as f:\n",
    "            dictionary = pickle.load(f)\n",
    "\n",
    "        weights = np.load(gfile)\n",
    "        return dictionary, weights\n",
    "\n",
    "    elif not os.path.exists(cache):\n",
    "        os.makedirs(cache)\n",
    "\n",
    "    dictionary = Dictionary()\n",
    "    questions = ['train_balanced_questions.json', 'val_balanced_questions.json', 'testdev_balanced_questions.json',\n",
    "                 'test_balanced_questions.json']\n",
    "    \n",
    "    # Iterate through Question in Question Files and update Vocabulary\n",
    "    print('\\t[*] Creating Dictionary from GQA Questions...')\n",
    "    for qfile in questions:\n",
    "        qpath = os.path.join(gqa_q, qfile)\n",
    "        with open(qpath, 'r') as f:\n",
    "            examples = json.load(f)\n",
    "\n",
    "        for ex_key in examples:\n",
    "            ex = examples[ex_key]\n",
    "            dictionary.tokenize(ex['question'], add_word=True)\n",
    "    \n",
    "    # Load GloVe Embeddings\n",
    "    print('\\t[*] Loading GloVe Embeddings...')\n",
    "    with open(glove, 'r') as f:\n",
    "        entries = f.readlines()\n",
    "    \n",
    "    # Assert that we’re using the 300-Dimensional GloVe Embeddings\n",
    "    assert len(entries[0].split()) - 1 == 300, 'ERROR - Not using 300-dimensional GloVe Embeddings!'\n",
    "    \n",
    "    # Create Embedding Weights\n",
    "    weights = np.zeros((len(dictionary.idx2word), 300), dtype=np.float32)\n",
    "    \n",
    "    # Populate Embedding Weights\n",
    "    for entry in entries:\n",
    "        word_vec = entry.split()\n",
    "        word, vec = word_vec[0], list(map(float, word_vec[1:]))\n",
    "        if word in dictionary.word2idx:\n",
    "            weights[dictionary.word2idx[word]] = vec\n",
    "    \n",
    "    # Dump Dictionary and Weights to file\n",
    "    with open(dfile, 'wb') as f:\n",
    "        pickle.dump(dictionary, f)\n",
    "    np.save(gfile, weights)\n",
    "    \n",
    "    # Return Dictionary and Weights\n",
    "    return dictionary, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sUrELqnyewKy"
   },
   "source": [
    "### 2. Preprocessing Answer Data\n",
    "\n",
    "Assemble dictionaries mapping answer strings to indices and vice-versa, for priming the Softmax in the final layer of the BUTD model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAbpPhF7fqJK"
   },
   "outputs": [],
   "source": [
    "# Create Mapping from Answers to Labels\n",
    "def gqa_create_answers(gqa_q='data/GQA-Questions', cache='data/GQA-Cache'):\n",
    "    # Create File Paths and Load from Disk (if cached)\n",
    "    dfile = os.path.join(cache, 'answers.pkl')\n",
    "    if os.path.exists(dfile):\n",
    "        with open(dfile, 'rb') as f:\n",
    "            ans2label, label2ans = pickle.load(f)\n",
    "\n",
    "        return ans2label, label2ans\n",
    "\n",
    "    ans2label, label2ans = {}, []\n",
    "    questions = ['train_balanced_questions.json', 'val_balanced_questions.json', 'testdev_balanced_questions.json']\n",
    "    \n",
    "    # Iterate through Answer in Question Files and update Mapping\n",
    "    print('\\t[*] Creating Answer Labels from GQA Question/Answers...')\n",
    "    for qfile in questions:\n",
    "        qpath = os.path.join(gqa_q, qfile)\n",
    "        with open(qpath, 'r') as f:\n",
    "            examples = json.load(f)\n",
    "\n",
    "        for ex_key in examples:\n",
    "            ex = examples[ex_key]\n",
    "            if not ex['answer'].lower() in ans2label:\n",
    "                ans2label[ex['answer'].lower()] = len(ans2label)\n",
    "                label2ans.append(ex['answer'])\n",
    "    \n",
    "    # Dump Dictionaries to File\n",
    "    with open(dfile, 'wb') as f:\n",
    "        pickle.dump((ans2label, label2ans), f)\n",
    "\n",
    "    return ans2label, label2ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFvwe-jrf6K6"
   },
   "source": [
    "### 3. Preprocessing Image Features\n",
    "\n",
    "Reads in a tsv file with pre-trained bottom up attention features and writes them to hdf5 file. Additionally builds image ID --> Feature IDX Mapping. Note that we use a version of Faster-RCNN that returns a *fixed 36 objects* (non-adaptive).\n",
    "\n",
    "**Note**: Bottom-Up Features courtesy of [LXMERT](https://github.com/airsplay/lxmert) via [Peter Anderson's Original Bottom-Up Top-Down Implementation](https://github.com/peteanderson80/bottom-up-attention).\n",
    "\n",
    "Hierarchy of HDF5 File:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'image_features': num_images x num_boxes x 2048,\n",
    "    'image_spatials': num_images x num_boxes x 6,\n",
    "    'image_bb': num_images x num_boxes x 4\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "erKUGchpf3zt"
   },
   "outputs": [],
   "source": [
    "# Set CSV Field Size Limit (Big TSV Files...)\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "FIELDNAMES = [\"img_id\", \"img_h\", \"img_w\", \"objects_id\", \"objects_conf\", \"attrs_id\", \"attrs_conf\", \"num_boxes\", \"boxes\",\n",
    "              \"features\"]\n",
    "NUM_FIXED_BOXES = 36\n",
    "FEATURE_LENGTH = 2048\n",
    "\n",
    "# Iterate through BUTD TSV and Build HDF5 Files with Bounding Box Features, Image ID –> IDX Mappings\n",
    "def gqa_create_image_features(gqa_f='data/GQA-Features', cache='data/GQA-Cache'):\n",
    "    print('\\t[*] Setting up HDF5 Files for Image/Object Features...')\n",
    "    \n",
    "    # Create Trackers for Image IDX –> Index\n",
    "    trainval_indices, testdev_indices = {}, {}\n",
    "    tv_file = os.path.join(cache, 'trainval36.hdf5')\n",
    "    td_file = os.path.join(cache, 'testdev36.hdf5')\n",
    "\n",
    "    tv_idxfile = os.path.join(cache, 'trainval36_img2idx.pkl')\n",
    "    td_idxfile = os.path.join(cache, 'testdev36_img2idx.pkl')\n",
    "\n",
    "    if os.path.exists(tv_file) and os.path.exists(td_file) and os.path.exists(tv_idxfile) and \\\n",
    "            os.path.exists(td_idxfile):\n",
    "\n",
    "        with open(tv_idxfile, 'rb') as f:\n",
    "            trainval_indices = pickle.load(f)\n",
    "\n",
    "        with open(td_idxfile, 'rb') as f:\n",
    "            testdev_indices = pickle.load(f)\n",
    "\n",
    "        return trainval_indices, testdev_indices\n",
    "\n",
    "    with h5py.File(tv_file, 'w') as h_trainval, h5py.File(td_file, 'w') as h_testdev:\n",
    "        # Get Number of Images in each Split\n",
    "        with open(os.path.join(gqa_f, 'vg_gqa_obj36.tsv'), 'r') as f:\n",
    "            ntrainval = len(f.readlines())\n",
    "\n",
    "        with open(os.path.join(gqa_f, 'gqa_testdev_obj36.tsv'), 'r') as f:\n",
    "            ntestdev = len(f.readlines())\n",
    "        \n",
    "        # Setup HDF5 Files\n",
    "        trainval_img_features = h_trainval.create_dataset('image_features', (ntrainval, NUM_FIXED_BOXES,\n",
    "                                                                             FEATURE_LENGTH), 'f')\n",
    "        trainval_img_bb = h_trainval.create_dataset('image_bb', (ntrainval, NUM_FIXED_BOXES, 4), 'f')\n",
    "        trainval_spatial_features = h_trainval.create_dataset('spatial_features', (ntrainval, NUM_FIXED_BOXES, 6), 'f')\n",
    "\n",
    "        testdev_img_features = h_testdev.create_dataset('image_features', (ntestdev, NUM_FIXED_BOXES, FEATURE_LENGTH),\n",
    "                                                        'f')\n",
    "        testdev_img_bb = h_testdev.create_dataset('image_bb', (ntestdev, NUM_FIXED_BOXES, 4), 'f')\n",
    "        testdev_spatial_features = h_testdev.create_dataset('spatial_features', (ntestdev, NUM_FIXED_BOXES, 6), 'f')\n",
    "        \n",
    "        # Start Iterating through TSV\n",
    "        print('\\t[*] Reading Train-Val TSV File and Populating HDF5 File...')\n",
    "        trainval_counter, testdev_counter = 0, 0\n",
    "        with open(os.path.join(gqa_f, 'vg_gqa_obj36.tsv'), 'r') as tsv:\n",
    "            reader = csv.DictReader(tsv, delimiter='\\t', fieldnames=FIELDNAMES)\n",
    "            for item in reader:\n",
    "                item['num_boxes'] = int(item['num_boxes'])\n",
    "                image_id = item['img_id']\n",
    "                image_w = float(item['img_w'])\n",
    "                image_h = float(item['img_h'])\n",
    "                bb = np.frombuffer(base64.b64decode(item['boxes']), dtype=np.float32).reshape((item['num_boxes'], -1))\n",
    "\n",
    "                box_width = bb[:, 2] - bb[:, 0]\n",
    "                box_height = bb[:, 3] - bb[:, 1]\n",
    "                scaled_width = box_width / image_w\n",
    "                scaled_height = box_height / image_h\n",
    "                scaled_x = bb[:, 0] / image_w\n",
    "                scaled_y = bb[:, 1] / image_h\n",
    "\n",
    "                scaled_width = scaled_width[..., np.newaxis]\n",
    "                scaled_height = scaled_height[..., np.newaxis]\n",
    "                scaled_x = scaled_x[..., np.newaxis]\n",
    "                scaled_y = scaled_y[..., np.newaxis]\n",
    "\n",
    "                spatial_features = np.concatenate(\n",
    "                    (scaled_x,\n",
    "                     scaled_y,\n",
    "                     scaled_x + scaled_width,\n",
    "                     scaled_y + scaled_height,\n",
    "                     scaled_width,\n",
    "                     scaled_height),\n",
    "                    axis=1)\n",
    "\n",
    "                trainval_indices[image_id] = trainval_counter\n",
    "                trainval_img_bb[trainval_counter, :, :] = bb\n",
    "                trainval_img_features[trainval_counter, :, :] = \\\n",
    "                    np.frombuffer(base64.b64decode(item['features']), dtype=np.float32).reshape((item['num_boxes'], -1))\n",
    "                trainval_spatial_features[trainval_counter, :, :] = spatial_features\n",
    "                trainval_counter += 1\n",
    "\n",
    "        print('\\t[*] Reading Test-Dev TSV File and Populating HDF5 File...')\n",
    "        with open(os.path.join(gqa_f, 'gqa_testdev_obj36.tsv'), 'r') as tsv:\n",
    "            reader = csv.DictReader(tsv, delimiter='\\t', fieldnames=FIELDNAMES)\n",
    "            for item in reader:\n",
    "                item['num_boxes'] = int(item['num_boxes'])\n",
    "                image_id = item['img_id']\n",
    "                image_w = float(item['img_w'])\n",
    "                image_h = float(item['img_h'])\n",
    "                bb = np.frombuffer(base64.b64decode(item['boxes']), dtype=np.float32).reshape((item['num_boxes'], -1))\n",
    "\n",
    "                box_width = bb[:, 2] - bb[:, 0]\n",
    "                box_height = bb[:, 3] - bb[:, 1]\n",
    "                scaled_width = box_width / image_w\n",
    "                scaled_height = box_height / image_h\n",
    "                scaled_x = bb[:, 0] / image_w\n",
    "                scaled_y = bb[:, 1] / image_h\n",
    "\n",
    "                scaled_width = scaled_width[..., np.newaxis]\n",
    "                scaled_height = scaled_height[..., np.newaxis]\n",
    "                scaled_x = scaled_x[..., np.newaxis]\n",
    "                scaled_y = scaled_y[..., np.newaxis]\n",
    "\n",
    "                spatial_features = np.concatenate(\n",
    "                    (scaled_x,\n",
    "                     scaled_y,\n",
    "                     scaled_x + scaled_width,\n",
    "                     scaled_y + scaled_height,\n",
    "                     scaled_width,\n",
    "                     scaled_height),\n",
    "                    axis=1)\n",
    "\n",
    "                testdev_indices[image_id] = testdev_counter\n",
    "                testdev_img_bb[testdev_counter, :, :] = bb\n",
    "                testdev_img_features[testdev_counter, :, :] = \\\n",
    "                    np.frombuffer(base64.b64decode(item['features']), dtype=np.float32).reshape((item['num_boxes'], -1))\n",
    "                testdev_spatial_features[testdev_counter, :, :] = spatial_features\n",
    "                testdev_counter += 1\n",
    "    \n",
    "    # Dump TrainVal and TestDev Indices to File\n",
    "    with open(tv_idxfile, 'wb') as f:\n",
    "        pickle.dump(trainval_indices, f)\n",
    "\n",
    "    with open(td_idxfile, 'wb') as f:\n",
    "        pickle.dump(testdev_indices, f)\n",
    "\n",
    "    return trainval_indices, testdev_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8XERBIciHXt"
   },
   "source": [
    "### 4. Dataset Assembly\n",
    "\n",
    "Define GQA Feature Dataset `torch.Dataset`, with utilities for loading image features from HDF5 files, and tensorizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqF1MJyuhCTg"
   },
   "outputs": [],
   "source": [
    "class GQAFeatureDataset(Dataset):\n",
    "    def __init__(self, dictionary, ans2label, label2ans, img2idx, gqa_q='data/GQA-Questions', cache='data/GQA-Cache',\n",
    "                 mode='train'):\n",
    "        super(GQAFeatureDataset, self).__init__()\n",
    "        self.dictionary, self.ans2label, self.label2ans, self.img2idx = dictionary, ans2label, label2ans, img2idx\n",
    "        \n",
    "        # Load HDF5 Image Features\n",
    "        print('\\t[*] Loading HDF5 Features...')\n",
    "        if mode in ['train', 'val']:\n",
    "            prefix = 'trainval'\n",
    "        else:\n",
    "            prefix = 'testdev'\n",
    "\n",
    "        self.v_dim, self.s_dim = 2048, 6\n",
    "        self.hf = h5py.File(os.path.join(cache, '%s36.hdf5' % prefix), 'r')\n",
    "        self.features = self.hf.get('image_features')\n",
    "        self.spatials = self.hf.get('spatial_features')\n",
    "        \n",
    "        # Create the Dataset Entries by Iterating through the Data\n",
    "        self.entries = load_dataset(self.img2idx, ans2label, gqa_q=gqa_q, mode=mode)\n",
    "\n",
    "        self.tokenize()\n",
    "        self.tensorize()\n",
    "    \n",
    "    # Tokenize and Front-Pad the Questions in the Dataset\n",
    "    def tokenize(self, max_length=40):\n",
    "        for entry in self.entries:\n",
    "            tokens = self.dictionary.tokenize(entry['question'], False)\n",
    "            tokens = tokens[:max_length]\n",
    "            if len(tokens) < max_length:\n",
    "                # Note that we pad in front of the sentence (GRU reads left-to-right)\n",
    "                padding = [self.dictionary.padding_idx] * (max_length - len(tokens))\n",
    "                tokens = padding + tokens\n",
    "            \n",
    "            assert len(tokens) == max_length, \"Tokenized & Padded Question != Max Length!\"\n",
    "            entry['q_token'] = tokens\n",
    "\n",
    "    def tensorize(self):\n",
    "        for entry in self.entries:\n",
    "            question = torch.from_numpy(np.array(entry['q_token']))\n",
    "            entry['q_token'] = question\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        entry = self.entries[index]\n",
    "        \n",
    "        # Get Features\n",
    "        features = torch.from_numpy(np.array(self.features[entry['image']]))\n",
    "        spatials = torch.from_numpy(np.array(self.spatials[entry['image']]))\n",
    "        question = entry['q_token']\n",
    "        target = entry['answer']\n",
    "\n",
    "        return features, spatials, question, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "# Load Dataset Entries\n",
    "def load_dataset(img2idx, ans2label, gqa_q='data/GQA-Questions', mode='train'):\n",
    "    question_path = os.path.join(gqa_q, '%s_balanced_questions.json' % mode)\n",
    "    with open(question_path, 'r') as f:\n",
    "        examples = json.load(f)\n",
    "\n",
    "    print('\\t[*] Creating GQA %s Entries...' % mode)\n",
    "    entries = []\n",
    "    for ex_key in sorted(examples):\n",
    "        entry = create_entry(examples[ex_key], ex_key, img2idx, ans2label)\n",
    "        entries.append(entry)\n",
    "\n",
    "    return entries\n",
    "\n",
    "def create_entry(example, qid, img2idx, ans2label):\n",
    "    img_id = example['imageId']\n",
    "    assert img_id in img2idx, 'Image ID not in Index!'\n",
    "\n",
    "    entry = {\n",
    "        'question_id': qid,\n",
    "        'image_id': img_id,\n",
    "        'image': img2idx[img_id],\n",
    "        'question': example['question'],\n",
    "        'answer': ans2label[example['answer'].lower()]\n",
    "    }\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ns6EFnJekWnI"
   },
   "source": [
    "## Model Definition\n",
    "\n",
    "In this section, we formally define the Bottom-Up Top-Down Model with product-based multi-modal fusion.\n",
    "\n",
    "This model is moderately different than that [originally proposed](https://arxiv.org/abs/1707.07998) and is instead inspired by the implementation by [Hengyuan Hu et. al.](https://github.com/hengyuan-hu/bottom-up-attention-vqa) with some minor tweaks around the handling of spatial features.\n",
    "\n",
    "It's also worth noting that this Model is built using the [PyTorch-Lightning](https://github.com/PyTorchLightning/pytorch-lightning) library -- an excellent resource for quickly prototyping research-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F_rkkzyrk1Kx"
   },
   "source": [
    "### Sub-Module Definitions\n",
    "\n",
    "Definitions of useful sub-components of the full BUTD Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D1hF2LSJkF0g"
   },
   "outputs": [],
   "source": [
    "# Simple utility class defining a fully connected network (multi-layer perceptron)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims, use_weight_norm=True):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            in_dim, out_dim = dims[i], dims[i + 1]\n",
    "            if use_weight_norm:\n",
    "                layers.append(weight_norm(nn.Linear(in_dim, out_dim), dim=None))\n",
    "            else:\n",
    "                layers.append(nn.Linear(in_dim, out_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # output: [bsz, *, dims[0]] –> [bsz, *, dims[-1]]\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UtUCy3gslF7t"
   },
   "outputs": [],
   "source": [
    "# Initialize an Embedding Matrix with the appropriate dimensions --> defines padding as last token in dict\n",
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, ntoken, dim, dropout=0.0):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.ntoken, self.dim = ntoken, dim\n",
    "\n",
    "        self.emb = nn.Embedding(ntoken + 1, dim, padding_idx=ntoken)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    # Set Embedding Weights from Numpy Array\n",
    "    def load_embeddings(self, weights):\n",
    "        assert weights.shape == (self.ntoken, self.dim)\n",
    "        self.emb.weight.data[:self.ntoken] = torch.from_numpy(weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : [bsz, seq_len] output: [bsz, seq_len, emb_dim]\n",
    "        return self.dropout(self.emb(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ocLJ3WqPlWN3"
   },
   "outputs": [],
   "source": [
    "# Initialize the RNN Question Encoder with the appropriate configuration\n",
    "class QuestionEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, nlayers=1, bidirectional=False, dropout=0.0, rnn='GRU'):\n",
    "        super(QuestionEncoder, self).__init__()\n",
    "        self.in_dim, self.hidden, self.nlayers, self.bidirectional = in_dim, hidden_dim, nlayers, bidirectional\n",
    "        self.rnn_type, self.rnn_cls = rnn, nn.GRU if rnn == 'GRU' else nn.LSTM\n",
    "        \n",
    "        # Initialize RNN\n",
    "        self.rnn = self.rnn_cls(self.in_dim, self.hidden, self.nlayers, bidirectional=self.bidirectional,\n",
    "                                dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [bsz, seq_len, emb_dim] -->\n",
    "        # output[0]: [bsz, seq_len, ndirections * hidden\n",
    "        # output[1]: [bsz, nlayers * ndirections, hidden]\n",
    "        output, hidden = self.rnn(x)  # Note that Hidden Defaults to 0\n",
    "        \n",
    "        # If not Bidirectional –> Just return last output state\n",
    "        if not self.bidirectional:\n",
    "            # output: [bsz, hidden]\n",
    "            return output[:, -1]\n",
    "        \n",
    "        # Otherwise, concat forward state for last element and backward state for first element\n",
    "        else:\n",
    "            # output: [bsz, 2 * hidden]\n",
    "            f, b = output[:, -1, :self.hidden], output[:, 0, self.hidden:]\n",
    "            return torch.cat([f, b], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QwuxLKfFmrt5"
   },
   "outputs": [],
   "source": [
    "# Initialize the Attention Mechanism with the appropriate fusion operation\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, image_dim, question_dim, hidden, dropout=0.2, use_weight_norm=True):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        # Attention w/ Product Fusion\n",
    "        self.image_proj = MLP([image_dim, hidden], use_weight_norm=use_weight_norm)\n",
    "        self.question_proj = MLP([question_dim, hidden], use_weight_norm=use_weight_norm)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = weight_norm(nn.Linear(hidden, 1), dim=None) if use_weight_norm else nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, image_features, question_emb):\n",
    "        # image_features: [bsz, k, image_dim = 2048]\n",
    "        # question_emb: [bsz, question_dim]\n",
    "\n",
    "        # Project both image and question embedding to hidden and repeat question_emb\n",
    "        num_objs = image_features.size(1)\n",
    "        image_proj = self.image_proj(image_features)\n",
    "        question_proj = self.question_proj(question_emb).unsqueeze(1).repeat(1, num_objs, 1)\n",
    "        \n",
    "        # Key: Fuse w/ Product\n",
    "        image_question = image_proj * question_proj\n",
    "        \n",
    "        # Dropout Joint Representation\n",
    "        joint_representation = self.dropout(image_question)\n",
    "        \n",
    "        # Compute Logits – Softmax\n",
    "        logits = self.linear(joint_representation)\n",
    "        return nn.functional.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-BkPbqFm8uj"
   },
   "source": [
    "### Key Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJw-lzAsnDsM"
   },
   "outputs": [],
   "source": [
    "class BUTD(pl.LightningModule):\n",
    "    def __init__(self, hparams, train_dataset, val_dataset, ans2label=None, label2ans=None):\n",
    "        super(BUTD, self).__init__()\n",
    "        \n",
    "        # Save Hyper-Parameters and Dataset\n",
    "        self.hparams = hparams\n",
    "        self.train_dataset, self.val_dataset = train_dataset, val_dataset\n",
    "        self.ans2label, self.label2ans = ans2label, label2ans\n",
    "        \n",
    "        # Build Model\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Build Word Embeddings (for Questions)\n",
    "        self.w_emb = WordEmbedding(ntoken=self.train_dataset.dictionary.ntoken, dim=self.hparams.emb_dim,\n",
    "                                   dropout=self.hparams.emb_dropout)\n",
    "        \n",
    "        # Build Question Encoder\n",
    "        self.q_enc = QuestionEncoder(in_dim=self.hparams.emb_dim, hidden_dim=self.hparams.hidden,\n",
    "                                     nlayers=self.hparams.rnn_layers, bidirectional=self.hparams.bidirectional,\n",
    "                                     dropout=self.hparams.q_dropout, rnn=self.hparams.rnn)\n",
    "        \n",
    "        # Build Attention Mechanism\n",
    "        self.att = Attention(image_dim=self.train_dataset.v_dim + 6, question_dim=self.q_enc.hidden,\n",
    "                             hidden=self.hparams.hidden, dropout=self.hparams.attention_dropout,\n",
    "                             use_weight_norm=self.hparams.weight_norm)\n",
    "        \n",
    "        # Build Projection Networks\n",
    "        self.q_project = MLP([self.q_enc.hidden, self.hparams.hidden], use_weight_norm=self.hparams.weight_norm)\n",
    "        self.img_project = MLP([self.train_dataset.v_dim + 6, self.hparams.hidden],\n",
    "                               use_weight_norm=self.hparams.weight_norm)\n",
    "        \n",
    "        # Build Answer Classifier\n",
    "        self.ans_classifier = nn.Sequential(*[\n",
    "            weight_norm(nn.Linear(self.hparams.hidden, 2 * self.hparams.hidden), dim=None)\n",
    "            if self.hparams.weight_norm else nn.Linear(self.hparams.hidden, 2 * self.hparams.hidden),\n",
    "\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.hparams.answer_dropout),\n",
    "\n",
    "            weight_norm(nn.Linear(2 * self.hparams.hidden, len(self.ans2label)), dim=None)\n",
    "            if self.hparams.weight_norm else nn.Linear(2 * self.hparams.hidden, len(self.ans2label))\n",
    "        ])\n",
    "\n",
    "    def forward(self, image_features, spatial_features, question_features, indicator_features=None):\n",
    "        # image_features: [bsz, K, image_dim], spatial_features: [bsz, K, 6], question_features: [bsz, seq_len]\n",
    "\n",
    "        # Embed and Encode Question – [bsz, q_hidden]\n",
    "        w_emb = self.w_emb(question_features)\n",
    "        q_enc = self.q_enc(w_emb)\n",
    "        \n",
    "        # Create new Image Features -- Key: Concatenate Spatial Features!\n",
    "        if indicator_features is not None:\n",
    "            image_features = torch.cat([image_features, spatial_features, indicator_features], dim=2)\n",
    "        else:\n",
    "            image_features = torch.cat([image_features, spatial_features], dim=2)\n",
    "        \n",
    "        # Attend over Image Features and Create Image Encoding\n",
    "        # img_enc: [bsz, img_hidden]\n",
    "        att = self.att(image_features, q_enc)\n",
    "        img_enc = (image_features * att).sum(dim=1)\n",
    "        \n",
    "        # Project Image and Question Features –> [bsz, hidden]\n",
    "        q_repr = self.q_project(q_enc)\n",
    "        img_repr = self.img_project(img_enc)\n",
    "        \n",
    "        # Merge\n",
    "        joint_repr = q_repr * img_repr\n",
    "        \n",
    "        # Compute and Return Logits\n",
    "        return self.ans_classifier(joint_repr)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.hparams.opt == 'adamax':\n",
    "            return torch.optim.Adamax(self.parameters())\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.hparams.bsz, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.hparams.bsz)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        img, spatials, question, answer = train_batch\n",
    "        \n",
    "        # Run Forward Pass\n",
    "        logits = self.forward(img, spatials, question)\n",
    "        \n",
    "        # Compute Loss (Cross-Entropy)\n",
    "        loss = nn.functional.cross_entropy(logits, answer)\n",
    "        \n",
    "        # Compute Answer Accuracy\n",
    "        accuracy = torch.mean((logits.argmax(dim=1) == answer).float())\n",
    "        \n",
    "        # Set up Data to be Logged\n",
    "        log = {'train_loss': loss, 'train_acc': accuracy}\n",
    "\n",
    "        return {'loss': loss, 'train_loss': loss, 'train_acc': accuracy, 'progress_bar': log, 'log': log}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        # Outputs –> List of Individual Step Outputs\n",
    "        avg_loss = torch.stack([x['callback_metrics']['train_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['callback_metrics']['train_acc'] for x in outputs]).mean()\n",
    "\n",
    "        log = {'train_epoch_loss': avg_loss, 'train_epoch_acc': avg_acc}\n",
    "\n",
    "        return {'progress_bar': log, 'log': log}\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        img, spatials, question, answer = val_batch\n",
    "        \n",
    "        # Run Forward Pass\n",
    "        logits = self.forward(img, spatials, question)\n",
    "        \n",
    "        # Compute Loss (Cross-Entropy)\n",
    "        loss = nn.functional.cross_entropy(logits, answer)\n",
    "        \n",
    "        # Compute Answer Accuracy\n",
    "        accuracy = torch.mean((logits.argmax(dim=1) == answer).float())\n",
    "\n",
    "        return {'val_loss': loss, 'val_acc': accuracy}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # Outputs –> List of Individual Step Outputs\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "\n",
    "        log = {'val_loss': avg_loss, 'val_acc': avg_acc}\n",
    "        return {'progress_bar': log, 'log': log}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBSHedHcpQSr"
   },
   "source": [
    "## Logging\n",
    "\n",
    "We tap into [PyTorch-Lightning](https://pytorch-lightning.readthedocs.io/en/latest/) and its extensive Logging Capabilities and define our own simple logger to log metrics like training loss, training accuracy, validation loss, and validation accuracy to straightforward JSON files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xza-YEi7nwbS"
   },
   "outputs": [],
   "source": [
    "class MetricLogger(LightningLoggerBase):\n",
    "    def __init__(self, name, save_dir):\n",
    "        super(MetricLogger, self).__init__()\n",
    "        self._name, self._save_dir = name, os.path.join(save_dir, 'metrics')\n",
    "        \n",
    "        # Create Massive Dictionary to JSONify\n",
    "        self.events = {}\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    @property\n",
    "    def experiment(self):\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def version(self):\n",
    "        return 1.0\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_hyperparams(self, params):\n",
    "        # Params is an argparse.Namespace\n",
    "        self.events['hyperparams'] = vars(params)\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_metrics(self, metrics, step):\n",
    "        # Metrics is a dictionary of metric names and values\n",
    "        for metric in metrics:\n",
    "            if metric in self.events:\n",
    "                self.events[metric].append(metrics[metric])\n",
    "                self.events[\"%s_step\" % metric].append(step)\n",
    "            else:\n",
    "                self.events[metric] = [metrics[metric]]\n",
    "                self.events[\"%s_step\" % metric] = [step]\n",
    "\n",
    "    @rank_zero_only\n",
    "    def finalize(self, status):\n",
    "        # Optional. Any code that needs to be run after training\n",
    "        self.events['status'] = status\n",
    "\n",
    "        if not os.path.exists(self._save_dir):\n",
    "            os.makedirs(self._save_dir)\n",
    "\n",
    "        with open(os.path.join(self._save_dir, '%s-metrics.json' % self._name), 'w') as f:\n",
    "            json.dump(self.events, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eyv8NCAOp1Ch"
   },
   "source": [
    "## Bringing the Pieces Together\n",
    "\n",
    "Here, we bring all the pieces together, calling each of the 4 preprocessing steps, assembling the training and development datasets, and initializing and training the BUTD model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 715
    },
    "colab_type": "code",
    "id": "1qrjKdh9pe1J",
    "outputId": "7fd642f1-da55-467b-f22e-377670d71713"
   },
   "outputs": [],
   "source": [
    "# Preprocess Question Data - Return Dictionary and GloVe-initialized Embeddings\n",
    "print('\\n[*] Pre-processing GQA Questions...')\n",
    "dictionary, emb = gqa_create_dictionary_glove(gqa_q=args.gqa_questions, glove=args.glove, cache=args.gqa_cache)\n",
    "\n",
    "# Preprocess Answer Data\n",
    "print('\\n[*] Pre-processing GQA Answers...')\n",
    "ans2label, label2ans = gqa_create_answers(gqa_q=args.gqa_questions, cache=args.gqa_cache)\n",
    "\n",
    "# Create Image Features\n",
    "print('\\n[*] Pre-processing GQA BUTD Image Features')\n",
    "trainval_img2idx, testdev_img2idx = gqa_create_image_features(gqa_f=args.gqa_features, cache=args.gqa_cache)\n",
    "\n",
    "# Build Train and TestDev Datasets – Note here that we use the TestDev split of GQA instead of Val (as is common \n",
    "# practice) because of Visual Genome data leakage in the Validation Set\n",
    "print('\\n[*] Building GQA Train and TestDev Datasets...')\n",
    "train_dataset = GQAFeatureDataset(dictionary, ans2label, label2ans, trainval_img2idx, gqa_q=args.gqa_questions,\n",
    "                                  cache=args.gqa_cache, mode='train')\n",
    "\n",
    "dev_dataset = GQAFeatureDataset(dictionary, ans2label, label2ans, testdev_img2idx, gqa_q=args.gqa_questions,\n",
    "                                cache=args.gqa_cache, mode='testdev')\n",
    "\n",
    "# Create BUTD Module (and load Embeddings!)\n",
    "print('\\n[*] Initializing Bottom-Up Top-Down Model...')\n",
    "nn = BUTD(args, train_dataset, dev_dataset, ans2label, label2ans)\n",
    "nn.w_emb.load_embeddings(emb)\n",
    "\n",
    "# Setup Logger for PyTorch-Lightning\n",
    "mt_logger = MetricLogger(name=run_name, save_dir=args.checkpoint)\n",
    "\n",
    "# Saves the top-3 Checkpoints based on Validation Accuracy – feel free to change this metric to suit your needs\n",
    "checkpoint_callback = ModelCheckpoint(filepath=os.path.join(args.checkpoint, 'runs', run_name,\n",
    "                                                            'butd-{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}'),\n",
    "                                      monitor='val_acc', mode='max', save_top_k=3)\n",
    "\n",
    "# Check GPUs\n",
    "GPUS = 0\n",
    "if torch.cuda.is_available():\n",
    "    GPUS = 1\n",
    "\n",
    "# Create Pytorch-Lightning Trainer – run for the given number of epochs, with gradient clipping!\n",
    "trainer = pl.Trainer(default_root_dir=args.checkpoint, max_epochs=args.epochs, gradient_clip_val=args.gradient_clip,\n",
    "                     gpus=GPUS, benchmark=True, logger=mt_logger, checkpoint_callback=checkpoint_callback)\n",
    "\n",
    "# Fit and Profit!\n",
    "print('\\n[*] Training...\\n')\n",
    "trainer.fit(nn)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "annotated_gqa.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
